{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1BMt4ja28FM"
   },
   "source": [
    "#<b>Milestone 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCiPVBNbA0Wh"
   },
   "source": [
    "##<b>Problem Definition</b>\n",
    "**The context:** Why is this problem important to solve?<br>\n",
    "**The objectives:** What is the intended goal?<br>\n",
    "**The key questions:** What are the key questions that need to be answered?<br>\n",
    "**The problem formulation:** What is it that we are trying to solve using data science?\n",
    "\n",
    "## <b>Data Description </b>\n",
    "\n",
    "There are a total of 24,958 train and 2,600 test images (colored) that we have taken from microscopic images. These images are of the following categories:<br>\n",
    "\n",
    "\n",
    "**Parasitized:** The parasitized cells contain the Plasmodium parasite which causes malaria<br>\n",
    "**Uninfected:** The uninfected cells are free of the Plasmodium parasites<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGsTRO4XBWKn"
   },
   "source": [
    "## <b>Important Notes</b>\n",
    "\n",
    "- This notebook can be considered a guide to refer to while solving the problem. The evaluation will be as per the Rubric shared for each Milestone. Unlike previous courses, it does not follow the pattern of the graded questions in different sections. This notebook would give you a direction on what steps need to be taken in order to get a viable solution to the problem. Please note that this is just one way of doing this. There can be other 'creative' ways to solve the problem and we urge you to feel free and explore them as an 'optional' exercise. \n",
    "\n",
    "- In the notebook, there are markdowns cells called - Observations and Insights. It is a good practice to provide observations and extract insights from the outputs.\n",
    "\n",
    "- The naming convention for different variables can vary. Please consider the code provided in this notebook as a sample code.\n",
    "\n",
    "- All the outputs in the notebook are just for reference and can be different if you follow a different approach.\n",
    "\n",
    "- There are sections called **Think About It** in the notebook that will help you get a better understanding of the reasoning behind a particular technique/step. Interested learners can take alternative approaches if they want to explore different techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDM89XHyCxrA"
   },
   "source": [
    "### Accessing AWS Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "     |████████████████████████████████| 458.3 MB 4.8 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.36.2)\n",
      "Collecting opt-einsum~=3.3.0\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     |████████████████████████████████| 65 kB 784 kB/s             \n",
      "\u001b[?25hCollecting keras<2.7,>=2.6.0\n",
      "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 43.2 MB/s            \n",
      "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting absl-py~=0.10\n",
      "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
      "     |████████████████████████████████| 132 kB 63.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "     |████████████████████████████████| 5.6 MB 17.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.15.2)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.1 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.19.5)\n",
      "Collecting typing-extensions~=3.7.4\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "     |████████████████████████████████| 462 kB 51.8 MB/s            \n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
      "  Downloading grpcio-1.46.3-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "     |████████████████████████████████| 4.4 MB 55.9 MB/s            \n",
      "\u001b[?25hCollecting termcolor~=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gast==0.4.0\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: six~=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorflow) (1.15.0)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow) (1.5.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.0.2)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
      "     |████████████████████████████████| 152 kB 35.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (2.26.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 64.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow) (49.6.0.post20210108)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 47.6 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.7-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 11.9 MB/s            \n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 72.5 MB/s            \n",
      "\u001b[?25hCollecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting importlib-metadata>=4.4\n",
      "  Downloading importlib_metadata-4.8.3-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow) (0.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 75.7 MB/s            \n",
      "\u001b[?25hBuilding wheels for collected packages: clang, termcolor\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30702 sha256=e1074b4bae9cd3658813304d8c04e0ada72f21882e6066e3af70ad9389dc307a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=1bed1e3566c3d7941d27fe30e5e94cae259a2c41622045e740f1b87eed54b47e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built clang termcolor\n",
      "Installing collected packages: typing-extensions, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, gast, flatbuffers, clang, astunparse, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.0.1\n",
      "    Uninstalling typing-extensions-4.0.1:\n",
      "      Successfully uninstalled typing-extensions-4.0.1\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.7.0\n",
      "    Uninstalling importlib-metadata-3.7.0:\n",
      "      Successfully uninstalled importlib-metadata-3.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQi_degJC3dm",
    "outputId": "4cec091d-4b73-4ccd-90e5-297bc640bded"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/boto3/compat.py:88: PythonDeprecationWarning: Boto3 will no longer support Python 3.6 starting May 30, 2022. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.7 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_images.zip\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "bucket = 'kdfdlkfasdklfjaslkdfjkalsdjfdasklflakdjflks'\n",
    "subfolder = ''\n",
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()\n",
    "conn = boto3.client('s3')\n",
    "contents = conn.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "for f in contents:\n",
    "    print(f['Key'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UC8-yLUUCcWh"
   },
   "source": [
    "### <b>Loading libraries</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iixNDZWs1ZPL"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3d92146a4545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAvgPool2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#Importing libraries required to load the data\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, BatchNormalization, Dropout, Flatten, LeakyReLU, GlobalAvgPool2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "#to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Remove the limit from the number of displayed columns and rows. It helps to see the entire dataframe while printing it\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCqJk2XpCnJi"
   },
   "source": [
    "### <b>Let us load the data</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_syvBdMlDTsr"
   },
   "source": [
    "**Note:** \n",
    "- You must download the dataset from the link provided on Olympus and upload the same on your Google drive before executing the code in the next cell.\n",
    "- In case of any error, please make sure that the path of the file is correct as the path may be different for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "ufMU62DICjLV"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly91Y3NkY2xvdWQtbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvYWdhcmR1bm9fdWNzZF9lZHUvRVREWTVXdTI0ZTFMamktQUFUZTlBZHdCUWUwYURrNGZGWmwyVk9RV2RKeUFuZw/root/content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-c8ddf15cbb43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#The data is provided as a zip file so we need to extract the files from the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64)\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://api.onedrive.com/v1.0/shares/u!aHR0cHM6Ly91Y3NkY2xvdWQtbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvYWdhcmR1bm9fdWNzZF9lZHUvRVREWTVXdTI0ZTFMamktQUFUZTlBZHdCUWUwYURrNGZGWmwyVk9RV2RKeUFuZw/root/content'"
     ]
    }
   ],
   "source": [
    "#Storing the path of the data file from the Google drive\n",
    "#not sure if this path needs to be updated everytime SageMaker is run\n",
    "#path = path + '/cell_images.zip'\n",
    "\n",
    "#The data is provided as a zip file so we need to extract the files from the zip file\n",
    "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IW3fDq7gF4Hw"
   },
   "source": [
    "The extracted folder has different folders for train and test data which further contains the different sizes of images for parasitized and uninfected cells within the respective folder name. \n",
    "\n",
    "The size of all images must be the same and should be converted to 4D arrays so that they can be used as an input for the convolutional neural network. Also, we need to create the labels for both types of images to be able to train and test the model. \n",
    "\n",
    "Let's do the same for the training data first and then we will use the same code for the test data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SjK02uF1DXdW"
   },
   "outputs": [],
   "source": [
    "#Storing the path of the extracted \"train\" folder \n",
    "train_dir = '/Users/alexisgarduno/Documents/2021-2022/DataScience Bootcamp/Final-Project/cell_images/train'\n",
    "\n",
    "#Size of image so that each image has the same size\n",
    "SIZE = 64\n",
    "\n",
    "#Empty list to store the training images after they are converted to NumPy arrays\n",
    "train_images = []\n",
    "\n",
    "#Empty list to store the training labels (0 - uninfected, 1 - parasitized)\n",
    "train_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2JNY4tSvDXgn"
   },
   "outputs": [],
   "source": [
    "#We will run the same code for \"parasitized\" as well as \"uninfected\" folders within the \"train\" folder\n",
    "for folder_name in ['/parasitized/', '/uninfected/']:\n",
    "    \n",
    "    #Path of the folder\n",
    "    images_path = os.listdir(train_dir + folder_name)\n",
    "\n",
    "    for i, image_name in enumerate(images_path):\n",
    "        try:\n",
    "            #Opening each image using the path of that image\n",
    "            image = Image.open(train_dir + folder_name + image_name)\n",
    "\n",
    "            #Resizing each image to (64,64)\n",
    "            image = image.resize((SIZE, SIZE))\n",
    "\n",
    "            #Converting images to arrays and appending that array to the empty list defined above\n",
    "            train_images.append(np.array(image))\n",
    "\n",
    "            #Creating labels for parasitized and uninfected images\n",
    "            if folder_name=='/parasitized/':\n",
    "                train_labels.append(1)\n",
    "            else:\n",
    "                train_labels.append(0)\n",
    "        except Exception:\n",
    "            pass       \n",
    "\n",
    "#Converting lists to arrays\n",
    "train_images = np.array(train_images)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DfwXIsK_DXiv"
   },
   "outputs": [],
   "source": [
    "#Storing the path of the extracted \"test\" folder \n",
    "test_dir = '/Users/alexisgarduno/Documents/2021-2022/DataScience Bootcamp/Final-Project/cell_images/test'\n",
    "\n",
    "#Size of image so that each image has the same size (it must be same as the train image size)\n",
    "SIZE = 64\n",
    "\n",
    "#Empty list to store the testing images after they are converted to NumPy arrays\n",
    "test_images = []\n",
    "\n",
    "#Empty list to store the testing labels (0 - uninfected, 1 - parasitized)\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "My6pm3AFDXk9"
   },
   "outputs": [],
   "source": [
    "#We will run the same code for \"parasitized\" as well as \"uninfected\" folders within the \"test\" folder\n",
    "for folder_name in ['/parasitized/', '/uninfected/']:\n",
    "    \n",
    "    #Path of the folder\n",
    "    images_path = os.listdir(test_dir + folder_name)\n",
    "\n",
    "    for i, image_name in enumerate(images_path):\n",
    "        try:\n",
    "            #Opening each image using the path of that image\n",
    "            image = Image.open(test_dir + folder_name + image_name)\n",
    "            \n",
    "            #Resizing each image to (64,64)\n",
    "            image = image.resize((SIZE, SIZE))\n",
    "            \n",
    "            #Converting images to arrays and appending that array to the empty list defined above\n",
    "            test_images.append(np.array(image))\n",
    "            \n",
    "            #Creating labels for parasitized and uninfected images\n",
    "            if folder_name=='/parasitized/':\n",
    "                test_labels.append(1)\n",
    "            else:\n",
    "                test_labels.append(0)\n",
    "        except Exception:\n",
    "            pass       \n",
    "\n",
    "#Converting lists to arrays\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_rxFT9iH7pR"
   },
   "source": [
    "###<b> Checking the shape of train and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LA8HJmQp1hU5"
   },
   "outputs": [],
   "source": [
    "# shape of images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3AiiutGIEoy"
   },
   "source": [
    "###<b> Checking the shape of train and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ_uvmT61rvx"
   },
   "outputs": [],
   "source": [
    "# shape of labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3gtgoubINVX"
   },
   "source": [
    "#####<b> Observations and insights: _____\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFUMkif7IjlO"
   },
   "source": [
    "### <b>Check the minimum and maximum range of pixel values for train and test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_gJbhCbMIswX"
   },
   "outputs": [],
   "source": [
    "# try to use min and max function from numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_T8WWuSpI6Ih"
   },
   "source": [
    "#####<b> Observations and insights: _____\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywUFcLSCPIqz"
   },
   "source": [
    "###<b> Count the number of values in both uninfected and parasitized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xcEb9liBPRlu"
   },
   "outputs": [],
   "source": [
    "# try to use value_counts to count the values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U71xb2XJe0t"
   },
   "source": [
    "###<b>Normalize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqzvvNS9IvsP"
   },
   "outputs": [],
   "source": [
    "# try to normalize the train and test images by dividing it by 255 and convert them to float32 using astype function\n",
    "train_images = (___________).astype('float32')\n",
    "test_images = (______________).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAT1eVTrKiDy"
   },
   "source": [
    "#####<b> Observations and insights: _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG4meNZBKZ5r"
   },
   "source": [
    "###<b> Plot to check if the data is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8N4qohCFIvvI"
   },
   "outputs": [],
   "source": [
    "# you are free to use bar plot or pie-plot or count plot, etc. to plot the labels of train and test data and check if they are balanced\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGfRJaffLI-C"
   },
   "source": [
    "#####<b> Observations and insights: _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16-Er2edMzsD"
   },
   "source": [
    "### <b>Data Exploration</b>\n",
    "Let's visualize the images from the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjhRVQ3-2Pa5"
   },
   "outputs": [],
   "source": [
    "# This code will help you in visualizing both the parasitized and uninfected images\n",
    "np.random.seed(42)\n",
    "plt.figure(1 , figsize = (16 , 16))\n",
    "\n",
    "for n in range(1, 17):\n",
    "    plt.subplot(4, 4, n)\n",
    "    index = int(np.random.randint(0, train_images.shape[0], 1))\n",
    "    if train_labels[index] == 1: \n",
    "        plt.title('parasitized')\n",
    "    else:\n",
    "        plt.title('uninfected')\n",
    "    plt.imshow(train_images[index])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OnyS_9IcCZLp"
   },
   "source": [
    "#####<b> Observations and insights: _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ay4oo5HrTDha"
   },
   "source": [
    "###<b> Similarly visualize the images with subplot(6,6) and figsize=(12,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKmncZ-fTAdD"
   },
   "outputs": [],
   "source": [
    "# Hint: Have a keen look into the number of iterations that the for loop should iterate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTG8UaNNNNso"
   },
   "source": [
    "#####<b>Observations and insights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifjZ2G0YN20r"
   },
   "source": [
    "###<b> Plotting the mean images for parasitized and uninfected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yN9X0620Iv1a"
   },
   "outputs": [],
   "source": [
    "# function to find the mean\n",
    "def find_mean_img(full_mat, title):\n",
    "    # calculate the average\n",
    "    mean_img = np.mean(full_mat, axis = 0)[0]\n",
    "    # reshape it back to a matrix\n",
    "    plt.imshow(mean_img)\n",
    "    plt.title(f'Average {title}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return mean_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ4xRQfUOMtm"
   },
   "source": [
    "<b> Mean image for parasitized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SlgCf1-x2gli"
   },
   "outputs": [],
   "source": [
    "# If the label=1 then the image is parasitised and if the label=0 then the image is uninfected\n",
    "parasitized_data=[]                                # Create a list to store the parasitized data\n",
    "for img, label in zip(train_images, train_labels):\n",
    "        if label==1:\n",
    "              parasitized_data.append([img])          \n",
    "\n",
    "parasitized_mean = find_mean_img(np.array(parasitized_data), 'Parasitized')   # find the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BkRt1rYIKE24"
   },
   "source": [
    "<b> Mean image for uninfected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji4w0okiIv6o"
   },
   "outputs": [],
   "source": [
    "# Similarly write the code to find the mean image of uninfected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lp9NP8yqPA69"
   },
   "source": [
    "#####<b> Observations and insights: _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzBagGEzHMHB"
   },
   "source": [
    "### <b>Converting RGB to HSV of Images using OpenCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJFCdE77H1Sg"
   },
   "source": [
    "####<b> Converting the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "32iwgGz7Iv-E"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "gfx=[]   # to hold the HSV image array\n",
    "for i in np.arange(0,100,1):\n",
    "  a=cv2.cvtColor(train_images[i],cv2.COLOR_BGR2HSV)\n",
    "  gfx.append(a)\n",
    "gfx=np.array(gfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3mr2iWd2pMK"
   },
   "outputs": [],
   "source": [
    "viewimage=np.random.randint(1,100,5)\n",
    "fig,ax=plt.subplots(1,5,figsize=(18,18))\n",
    "for t,i in zip(range(5),viewimage):\n",
    "  Title=train_labels[i]\n",
    "  ax[t].set_title(Title)\n",
    "  ax[t].imshow(gfx[i])\n",
    "  ax[t].set_axis_off()\n",
    "  fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9a7cFllH794"
   },
   "source": [
    "####<b> Converting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-kUOULrGpBA"
   },
   "outputs": [],
   "source": [
    "# Similarly you can visualize for the images in the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xq4_3hv6J3ad"
   },
   "source": [
    "#####<b>Observations and insights: _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7x9uDxuJur7"
   },
   "source": [
    "###<b> Processing Images using Gaussian Blurring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgkB_-J2KhBQ"
   },
   "source": [
    "####<b> Gaussian Blurring on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7GtZThUI6ug"
   },
   "outputs": [],
   "source": [
    "gbx=[]  ## to hold the blurred images\n",
    "for i in np.arange(0,100,1):\n",
    "  b= cv2.GaussianBlur(train_images[i], (5, 5), 0)\n",
    "  gbx.append(b)\n",
    "gbx=np.array(gbx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9qa58ep207S"
   },
   "outputs": [],
   "source": [
    "viewimage=np.random.randint(1,100,5)\n",
    "fig,ax=plt.subplots(1,5,figsize=(18,18))\n",
    "for t,i in zip(range(5),viewimage):\n",
    "  Title=train_labels[i]\n",
    "  ax[t].set_title(Title)\n",
    "  ax[t].imshow(gbx[i])\n",
    "  ax[t].set_axis_off()\n",
    "  fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTsJkRtCKlYZ"
   },
   "source": [
    "####<b> Gaussian Blurring on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XbmMGySOKblh"
   },
   "outputs": [],
   "source": [
    "# Similarly you can apply Gaussian blurring for the images in the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkpR8tQFKplr"
   },
   "source": [
    "#####**Observations and insights: _____**\n",
    "\n",
    "**Think About It:** Would blurring help us for this problem statement in any way? What else can we try?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1dIGd83e5Th"
   },
   "source": [
    "## **Proposed approach**\n",
    "\n",
    "**Potential techniques:** What different techniques should be explored?<br>\n",
    "**Overall solution design:** What is the potential solution design?<br>\n",
    "**Measures of success:** What are the key measures of success to compare different techniques?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEwtI9EIfY5v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Reference_Notebook_Milestone1_DL.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
